{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhqVSofWCeHl"
   },
   "source": [
    "# Assignment 2 #\n",
    "### Due: Friday, September 22 to be submitted via Canvas by 11:59 pm ###\n",
    "### Total points: **85** ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_oWED2hlDCVs"
   },
   "source": [
    "Your homework should be written in a python notebook. If you prefer, you can work in groups of two. For any question that requires a handwritten solution, you may upload scanned images of your solution in the notebook or attach them to the assignment . You may write your solution using markdown as well.\n",
    "\n",
    "### Note that:\n",
    "###1. Only one student per group needs to submit the assignment on Canvas;\n",
    "###2. Make sure to include both students' names, UT EIDs and homework group number in your submitted notebook;\n",
    "###3. Please make sure your code runs, the graphs and figures are displayed in your notebook before submitting. (%matplotlib inline)\n",
    "### 4. Late submissions receive 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BElYjEVrDW_C"
   },
   "source": [
    "# Question 1: Bias and Variance (**10 pts**)\n",
    "\n",
    "(a). (**4 pts**) Describe the difference between model bias and the bias of a point estimator.\n",
    "\n",
    "(b) (**6 pts**). How can you use a learning curve to determine whether a model is overfitting  (for a given sample size)? Discuss this with respect to the observed train and validation error curves. How does your answer change if the model you are trying to determine is underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Difference Between Model Bias and Bias of a Point Estimator**\n",
    "**Model Bias** - \n",
    "**Bias of a Point Estimator** - The difference between the expected value of the estimator and the value of the parameter being estimated. The model will be considered non bias if the expected value and the original value is same.\n",
    "\n",
    "\n",
    "\n",
    "**Model Bias (Statistical/Machine Learning):**\n",
    "\n",
    "Model bias is when the model constantly predicts and errounous output or predictions. It generally occurs when a model consistently predicts values which are different from the original values in a specific It occurs when a model consistently predicts values that are different from the true values in a particular direction.\n",
    "Model bias can result from various sources, including model assumptions, simplifications, or limitations. For example, if a linear regression model assumes a linear relationship between variables when the true relationship is nonlinear, it will exhibit model bias.\n",
    "Reducing model bias often involves refining the model architecture, incorporating more relevant features, or adjusting model hyperparameters to better capture the underlying data distribution.\n",
    "Bias of a Point Estimator (Statistics):\n",
    "\n",
    "The bias of a point estimator is a statistical concept that pertains to the extent to which an estimator consistently overestimates or underestimates the true population parameter it is estimating. In this context, an estimator is a statistic used to estimate a parameter of a population (e.g., the sample mean as an estimator for the population mean).\n",
    "A point estimator is considered unbiased if, on average, it produces estimates that are equal to the true population parameter. If it consistently overestimates or underestimates, it is said to have bias.\n",
    "Mathematically, the bias of a point estimator is defined as the expected difference between the estimator's values and the true parameter value. If this expected difference is zero, the estimator is unbiased.\n",
    "In summary, the key differences between model bias and the bias of a point estimator are:\n",
    "\n",
    "Context: Model bias is primarily a concern in machine learning and statistical modeling, where the focus is on predicting outcomes or modeling data, while the bias of a point estimator is a concept in traditional statistics used to assess the quality of parameter estimates.\n",
    "\n",
    "Type of Measurement: Model bias assesses the accuracy of predictions or outputs generated by a model, whereas the bias of a point estimator assesses the accuracy of estimates for population parameters.\n",
    "\n",
    "Measurement Scale: Model bias is typically assessed using metrics like Mean Absolute Error (MAE) or Mean Squared Error (MSE), while the bias of a point estimator is quantified by comparing estimated values to the true parameter values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SODU_RZZ56qR"
   },
   "source": [
    "# Question 2: Bias-Variance Exploration (**20 points**)\n",
    "Consider a function $g$ such that $g(x)=2sin(\\pi\n",
    "x)cos(3\\pi x^2)$. We will try to fit this function using a simple (binning based) piecewise constant function with varying number of bins. Here the number of bins controls the model complexity.\n",
    "\n",
    "Note: problem below uses the statistics notation: target is y and your estimates are $y_{hat}$.\n",
    "\n",
    "\n",
    "a) (2 points) Generate a dataset $(D = X,Y)$ by generating a set of 1-dimensional x's and y's in the following way -\n",
    "*  $x_i = \\frac{i}{2000}$ for each $i$ in $1, 2, 3 \\ldots 2000$.\n",
    "* Then, for each of the $x_i$'s obtain $y_i = 2sin(\\pi x_i)cos(3\\pi x_i^2)+ N(0,1)$ where $N(0,1)$ denotes the normal distribution with mean 0 and variance 1.\n",
    "\n",
    "Create 10 such datasets by independently repeating the entire process 10 times. Plot any one of the generated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAYyW5P8spKr"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def generate_data(num_points):\n",
    "  x = np.arange(1, num_points + 1) / num_points\n",
    "  ### START CODE ###\n",
    "  # Compute y according to the labeling function g\n",
    "  y = None\n",
    "  ### END CODE ###\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKK50lmhspt9"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for j in range(10):\n",
    "  datasets[j] = {}\n",
    "  datasets[j]['X'], datasets[j]['Y'] = generate_data(2000)\n",
    "\n",
    "### START CODE ###\n",
    "# Plot y v.s. x of any one of the dataset\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOMyze47BSbr"
   },
   "source": [
    "\n",
    "b) (5 points) Consider an estimator of function $g$, $f(x)$ that divides the x's into $K$ bins such that there are $m = n/K$ data points in each bin and the predicted y for all points in that bin is the mean of all the $y$'s in the bin.\n",
    "$$\n",
    "f(x) = \\sum_{j=1}^{K} \\bar{y}_j \\mathbb{1}(x \\in \\text{bin } j)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\bar{y}_j = \\frac{1}{m}\\sum_{i=j*m}^{(j+1)*m-1} y_i.\n",
    "$$\n",
    "Implement this estimator. Obtain $Y_{hat} = f(x)$ for one of the datasets created above by using $K=25$ and plot the predicted $Y_{hat}$ along with $X$ and $Y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWmWJR1vsr5h"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "# Implement Estimator\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LCJ7xh7sthu"
   },
   "outputs": [],
   "source": [
    "### START CODE ###\n",
    "# Plot predicted y_{hat} along with X and Y\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btrYRHTqBYVo"
   },
   "source": [
    "c) (3 points) What do you think will happen to the MSE, the bias and the variance as the number of bins $K$ is increased?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJPM7IHPBdFz"
   },
   "source": [
    "\n",
    "d) (5 points) For each dataset, vary the number of bins $K \\in \\{2,5,10,25,50,100,250, 500, 1000\\}$ and obtain the predictions. Also, for any one of the datasets (your choice), compute and plot the MSE obtained with varying $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOJKqQqPs11k"
   },
   "outputs": [],
   "source": [
    "mse_scores = []\n",
    "K = [2,5,10,25,50,100,250,500,1000]\n",
    "for n in K:\n",
    "  ### START CODE ###\n",
    "  # For the chosen dataset, compute y_hat\n",
    "  ### END CODE ###\n",
    "\n",
    "  ### START CODE ###\n",
    "  # Compute MSE\n",
    "  mse = None\n",
    "  ### END CODE ###\n",
    "  mse_scores.append(mse)\n",
    "plt.plot(K,mse_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H26-1_SoBj44"
   },
   "source": [
    "e) (5 points) Recall the definitions of the bias and variance, and now given the predictions obtained from 10 different datasets, compute the (sample) bias and variance of the model for each of the $K \\in \\{2,5,10,25,50,100,250,500,1000\\}$ and plot $K$  bias-squared vs $K$ and variance vs. $K$  on the same plot.\n",
    "We are using the term \"sample\" to remember that bias/variance are \"expected quantities\" that we are approximating by considering only 10 models, one per dataset.\n",
    "\n",
    "If y_pred denotes the 2000 x 10 2-D matrix of estimates for 2000 data points and obtained by the function estimated from the 10 datasets, use the following code to obtain the sample bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gmAcFx4-BkdX"
   },
   "outputs": [],
   "source": [
    "bias_vals = []\n",
    "var_vals = []\n",
    "K = [2,5,10,25,50,100,250,500,1000]\n",
    "pred_y = np.zeros((len(datasets[0]['Y']),10)) # axis 0: number of data points, axis 1: number of datasets (10)\n",
    "\n",
    "for n in K:\n",
    "  for j in datasets.keys():\n",
    "    x = datasets[j]['X']\n",
    "    y = datasets[j]['Y']\n",
    "    ### START CODE ###\n",
    "    # Estimate y_{hat}\n",
    "    y_hat = None\n",
    "    ### END CODE ###\n",
    "    pred_y[:,j] = y_hat\n",
    "\n",
    "  ### START CODE ###\n",
    "  # Compute the mean of the estimates over 10 datasets for each input x_{i}\n",
    "  avg_yhat = None # 1D array of shape (2000,)\n",
    "  # Compute the MSE between avg_yhat and y as bias\n",
    "  bias = None\n",
    "  # Compute the variance of the estimates over 10 datasets for each input x_{i}, then average over all inputs\n",
    "  var_yhat = None\n",
    "  ### END CODE ###\n",
    "  bias_vals.append(bias)\n",
    "  var_vals.append(var_yhat)\n",
    "plt.plot(K, bias_vals)\n",
    "plt.plot(K, var_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8osSuMrKNkbV"
   },
   "source": [
    "# Question 3: Stochastic Gradient Descent Improvements (**10 pts**)\n",
    "\n",
    "## Part 1. (**5 pts**) ##\n",
    "Read this [blog](https://medium.com/optimization-algorithms-for-deep-neural-networks/gradient-descent-with-momentum-dce805cd8de8) on medium and describe in your own words how momentum leads to a faster convergence of the loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymgkxoqCrbDr"
   },
   "source": [
    "## Part 2. (**5 pts**) ##\n",
    "Read this [blog](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461) on medium and explain in your own words the advantages of Mini-batch Stochastic Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au9dqqwEERjQ"
   },
   "source": [
    "# Question 4: Stochastic Gradient Descent (30 pts)\n",
    "\n",
    "## Part 1. (**10 pts**) Stochastic gradient descent derivation ##\n",
    "\n",
    "Use stochastic gradient descent to derive the coefficent updates (assuming squared loss is being used as the cost function) for the 4 coefficients $w_0, w_1, w_2, w_3$ in this model：\n",
    "\n",
    "$$ y = w_0 + w_1e^{-x_1} + w_2 x_1 + w_3x_1x_2 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aajnSTB-rbDr"
   },
   "source": [
    "## Part 2. (**20 pts**) Stochastic gradient descent coding ##\n",
    "\n",
    "Code an SGD solution in Python for this non-linear model$$ y = w_0 + w_1e^{-x_1} + w_2x_1 + w_3x_1x_2 $$  The template of the solution class is given. The init function of the class takes as input the learning rate, regularization constant and number of epochs. The fit method must take as input X, y. The predict method takes an X value (optionally, an array of values).\n",
    "\n",
    "a) (**15 pts**) Use the expression derived in part 1 to predict the data given in 'SGD_samples.csv', for 15 epochs, using learning rates: [0, .0001, .001, .01, 0.1, 1, 10, 100] and regularization (ridge regression) constants: [0,10,100]. For the best 2 combinations of learning_rate and regularization for SGD, plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) .\n",
    "\n",
    "b) (**5 pts**) Report the MSE of the two best combinations of learning rate and regularization constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hHHQcoWLY0f"
   },
   "outputs": [],
   "source": [
    "# Only use this code block if you are using Google Colab.\n",
    "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
    "from google.colab import files\n",
    "\n",
    "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
    "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6TbikpkLY20"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "class Regression:\n",
    "\n",
    "    def __init__(self, learning_rate, regularization, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.regularization = regularization\n",
    "        # initialize whichever variables you would need here\n",
    "        self.coef = np.zeros(4)\n",
    "\n",
    "    def sgd(self, gradient):\n",
    "        # Update the self.coef using SGD\n",
    "        ### START CODE ###\n",
    "        self.coef = None\n",
    "        ### END CODE ###\n",
    "\n",
    "    def fit(self, X, y, update_rule='sgd', plot=False):\n",
    "        mse = []\n",
    "        coefs = []\n",
    "        X = self.get_features(X)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            for i in range(X.shape[0]):\n",
    "                # Compute error\n",
    "                ### START CODE ###\n",
    "                ### END CODE ###\n",
    "\n",
    "                # Compute gradients\n",
    "                ### START CODE ###\n",
    "                ### END CODE ###\n",
    "\n",
    "                # Update weights\n",
    "                ### START CODE ###\n",
    "                ### END CODE ###\n",
    "\n",
    "            coefs.append(self.coef)\n",
    "            residuals = y - self.linearPredict(X)\n",
    "            mse.append(np.mean(residuals**2))\n",
    "\n",
    "        self.lowest_mse = mse[-1]\n",
    "        if plot == True:\n",
    "            plt.figure()\n",
    "            plt.plot(range(self.n_epoch),mse)\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.figure()\n",
    "            coefs = np.array(coefs)\n",
    "            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n",
    "            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n",
    "            plt.legend()\n",
    "            plt.xlabel('epoch')\n",
    "            plt.ylabel('parameter value')\n",
    "\n",
    "    def get_features(self, X):\n",
    "        '''\n",
    "        this output of this function can be used to compute the gradient in `fit`\n",
    "        '''\n",
    "        x = np.zeros((X.shape[0], 4))\n",
    "        x[:,0] = 1\n",
    "        x[:,1] = np.exp(-X[:,0])\n",
    "        x[:,2] = X[:,0]\n",
    "        x[:,3] = X[:,0]*X[:,1]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def linearPredict(self, X):\n",
    "        # Compute the dot product of self.coef and X\n",
    "        ### START CODE ###\n",
    "        return None\n",
    "        ### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nji0zmtkLY5B"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('SGD_samples.csv')\n",
    "X = np.array([data['x1'].values, data['x2'].values]).T\n",
    "y = data['y'].values\n",
    "n_epochs = 15\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "regularization = [0, 10, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKOQ-MnirbDs"
   },
   "outputs": [],
   "source": [
    "# Iterate through all combinations of learning rates and regularization strength\n",
    "# Use your Regression class to fit the data and record MSEs\n",
    "### START CODE ###\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aep9Gw-KrbDs"
   },
   "outputs": [],
   "source": [
    "# For the best two combinations, use the plot option in Regression.fit() to plot MSE and parameters as a function of epoch (15 epochs)\n",
    "### START CODE ###\n",
    "### END CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNFzAIHxrbDs"
   },
   "source": [
    "# Question 5: Visualizing Gradient Descent (**15 pts**) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lIXK948rbDs"
   },
   "source": [
    "## Part 1. **(10 pts)** Coding ##\n",
    "\n",
    "In this exercise, you are going to visualize four batch update steps of gradient descent for a  linear regression model with two parameters (i.e. weights, indicated by $\\theta$).\n",
    "\n",
    "The true target function is $t = \\theta_{0} + \\theta_{1}x$ with $\\theta_{0}=2$ and $\\theta_{1}=0.5$.\n",
    "\n",
    "Try the following two initializations:\n",
    "* $\\theta_{0}=0$ and $\\theta_{1}=0$\n",
    "* $\\theta_{0}=0$ and $\\theta_{1}=-4$\n",
    "\n",
    "and try the following three learning rates:\n",
    "* 0.5\n",
    "* 1\n",
    "* 2.1\n",
    "\n",
    "Therefore, there will be **six** combinations or settings to consider in total. For each setting, you will plot (a) the data and the changing linear regression fit and (b) the model parameters moving in the weight space after every update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l56rKW9hrbDs"
   },
   "outputs": [],
   "source": [
    "# Generate data\n",
    "np.random.seed(42)\n",
    "m = 20\n",
    "theta0_true = 2\n",
    "theta1_true = 0.5\n",
    "x = np.linspace(-1,1,m)\n",
    "y = theta0_true + theta1_true * x + np.random.normal(0, 0.2, size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0H_MsMVCrbDs"
   },
   "outputs": [],
   "source": [
    "def loss_func(theta0, theta1):\n",
    "    theta0 = np.atleast_3d(np.asarray(theta0))\n",
    "    theta1 = np.atleast_3d(np.asarray(theta1))\n",
    "    return np.average((y - model(x, theta0, theta1))**2, axis=2)/2\n",
    "\n",
    "def model(x, theta0, theta1):\n",
    "    return theta0 + theta1 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTks0r_TrbDs"
   },
   "outputs": [],
   "source": [
    "# Experiment with different initialization and learning rate combinations\n",
    "### START CODE ###\n",
    "init_list = [None]\n",
    "lr_list = [None]\n",
    "### END CODE ###\n",
    "\n",
    "# Left column shows the data and the changing linear regression models\n",
    "# Right column shows the model parameters moving over the loss landscape\n",
    "fig, ax = plt.subplots(nrows=len(init_list) * len(lr_list), ncols=2, figsize=(12, 36))\n",
    "\n",
    "for i, init in enumerate(init_list):\n",
    "    for j, lr in enumerate(lr_list):\n",
    "        row_idx = i * len(lr_list) + j\n",
    "        ax[row_idx][0].scatter(x, y, marker='x', s=40, color='k')\n",
    "        theta0_grid = np.linspace(-1,5,101)\n",
    "        theta1_grid = np.linspace(-5,5,101)\n",
    "        L_grid = loss_func(theta0_grid[np.newaxis,:,np.newaxis],\n",
    "                           theta1_grid[:,np.newaxis,np.newaxis])\n",
    "\n",
    "        # A labeled contour plot for the right column\n",
    "        X, Y = np.meshgrid(theta0_grid, theta1_grid)\n",
    "        contours = ax[row_idx][1].contour(X, Y, L_grid, 30)\n",
    "        ax[row_idx][1].clabel(contours)\n",
    "        # The target parameter values indicated on the loss function contour plot\n",
    "        ax[row_idx][1].scatter([theta0_true]*2,[theta1_true]*2,s=[50,10], color=['k','w'])\n",
    "\n",
    "        # Take N = 4 steps with learning rate alpha down the steepest gradient, starting at init\n",
    "        N = 4\n",
    "        theta = [init] # placeholder list for storing historical parameters\n",
    "        L = [loss_func(*theta[0])[0]] # placeholder list for storing historical loss values\n",
    "        for _ in range(N):\n",
    "            last_theta = theta[-1]\n",
    "            this_theta = np.empty((2,))\n",
    "            # Update theta\n",
    "            ### START CODE ### (2 lines of code)\n",
    "            this_theta[0] = None\n",
    "            this_theta[1] = None\n",
    "            ### END CODE ###\n",
    "            theta.append(this_theta)\n",
    "            L.append(loss_func(*this_theta))\n",
    "\n",
    "        # Annotate the loss function plot with coloured points indicating the\n",
    "        # parameters chosen and red arrows indicating the steps down the gradient.\n",
    "        # Also plot the fit function on the LHS data plot in a matching colour.\n",
    "        colors = ['b', 'g', 'm', 'c', 'orange']\n",
    "        ax[row_idx][0].plot(x, model(x, *theta[0]), color=colors[0], lw=2,\n",
    "                   label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[0]))\n",
    "        for k in range(1,N+1):\n",
    "            ax[row_idx][1].annotate('', xy=theta[k], xytext=theta[k-1],\n",
    "                           arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n",
    "                           va='center', ha='center')\n",
    "            ax[row_idx][0].plot(x, model(x, *theta[k]), color=colors[k], lw=2,\n",
    "                   label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[k]))\n",
    "        ax[row_idx][1].scatter(*zip(*theta), c=colors, s=40, lw=0)\n",
    "\n",
    "        # Labels and titles.\n",
    "        ax[row_idx][1].set_xlabel(r'$\\theta_0$')\n",
    "        ax[row_idx][1].set_ylabel(r'$\\theta_1$')\n",
    "        ax[row_idx][1].set_title(f'Loss function (Init:[{init[0]},{init[1]}], LR:{lr})')\n",
    "        ax[row_idx][0].set_xlabel(r'$x$')\n",
    "        ax[row_idx][0].set_ylabel(r'$y$')\n",
    "        ax[row_idx][0].set_title(f'Data and Fit (Init:[{init[0]},{init[1]}], LR:{lr})')\n",
    "        axbox = ax[row_idx][0].get_position()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOErdM90rbDs"
   },
   "source": [
    "## Part 2. **(5 pts)** ##\n",
    "For the experiment above, briefly summarize what you observed about the impact of (i) initialization and (ii) learning rate, on the evolution of the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aW2LJ44vsHSo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
